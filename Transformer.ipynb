{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPhv7fmYz0X6yCk/1zpo+U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arvishcdoshi/NN-Transformer/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HB4NFMQ74oDr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess text\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    return text\n",
        "\n",
        "file_path = \"hp_1.txt\"  # Ensure you have this file in your Colab or local directory\n",
        "text = load_data(file_path).lower()\n",
        "\n",
        "# Tokenization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(oov_token='') # Out-Of-Vocabulary token\n",
        "                                        # If a word not seen during training appears later, it will be replaced with\n",
        "                                        # Helps handle unknown words instead of ignoring them\n",
        "tokenizer.fit_on_texts([text]) # analyzes the input text and creates a word index (mapping of words to unique integers)\n",
        "total_words = len(tokenizer.word_index) + 1 #  0 is usually reserved for padding\n",
        "\n",
        "# Convert text to sequences\n",
        "input_sequences = []\n",
        "tokens = tokenizer.texts_to_sequences([text])[0] # converts the input text into a list of numbers based on the word index\n",
        "seq_length = 50  # Each input sequence contains 50 words\n",
        "\n",
        "# First seq_length tokens (input): Used for training the model.\n",
        "# Last token (target): Used as the label the model tries to predict.\n",
        "# so total of (50 + 1) in one input_sequence index\n",
        "\n",
        "for i in range(seq_length, len(tokens)):\n",
        "    input_sequences.append(tokens[i - seq_length:i + 1])\n",
        "\n",
        "# Pad sequences and split inputs/targets\n",
        "# after this X will have inputs and y will have label for those inputs\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length + 1, padding='pre'))\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "# One-hot encode the labels , note- there are other ways for\n",
        "# encoding like pre-trained word2vec encoding and so on\n",
        "\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model"
      ],
      "metadata": {
        "id": "qck2jY5e6NLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi Head Attention"
      ],
      "metadata": {
        "id": "YjHNYHWGn--j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
        "\n",
        "# We'll consider MultiHead attention like a layer.\n",
        "class MultiHeadAttention(Layer):\n",
        "  # When we've multi headed attention, the things we need are : no. of self-attention heads\n",
        "  # and the dimentions of the embedding.\n",
        "  def __init__(self, embed_dim, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.embed_dim = embed_dim # 512 ( Total embedding dimension : i.e 64*8 => check \"Attention is all you need\" paper for more info)\n",
        "    self.num_heads = num_heads # Suppose - 8\n",
        "    self.projection_dim = embed_dim // num_heads # 64\n",
        "\n",
        "    self.query_dense = Dense(embed_dim) # Wq\n",
        "    self.key_dense = Dense(embed_dim) # Wk\n",
        "    self.value_dense = Dense(embed_dim) # Wv\n",
        "\n",
        "    self.combine_dense = Dense(embed_dim)\n",
        "\n",
        "\n",
        "  def attention(self, query, key, value):\n",
        "    scores = tf.matmul(query, key, transpose_b=True) # Q * K Transpose\n",
        "    scores /= tf.math.sqrt(tf.cast(self.projection_dim, tf.float32)) # Convert int to float 32\n",
        "\n",
        "    attention_probs = tf.nn.softmax(scores, axis=-1) # softmax\n",
        "    return tf.matmul(attention_probs, value) # Probabs * V\n",
        "\n",
        "\n",
        "  # x - query, key, value\n",
        "  def split_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    # new shape - batch_size, num_of_words (seq_len), num_heads, projection\n",
        "    # shape we want - batch_size, num_heads, num of words (seq_len), projection\n",
        "    # batch_size of ( 8 heads of (4 words * 64 dimension))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value = inputs # shape -> batch_size, num_of_words (seq_len), embed_dim\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    query = self.split_heads(self.query_dense(query), batch_size)\n",
        "    key = self.split_heads(self.key_dense(key), batch_size)\n",
        "    value = self.split_heads(self.value_dense(value), batch_size)\n",
        "\n",
        "    attention = self.attention(query, key, value)\n",
        "    # shape i have -> batch_size, num_heads, num of words (seq_len), projection\n",
        "    # shape i want -> batch_size, num of words (seq_len), num_heads, projection\n",
        "    attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "\n",
        "    concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
        "    # How we concatenate above is :- For every batch, we merge all the heads into a single vector.\n",
        "\n",
        "    # Afterwords, we need to return the final output. Which is to combine the heads.\n",
        "    # So whatever attention we have, we're concatenating to get the final answer\n",
        "\n",
        "    return self.combine_dense(concat_attention)"
      ],
      "metadata": {
        "id": "VUBu1PBn6SGo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "zUrU6_0Zn6_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer"
      ],
      "metadata": {
        "id": "aV5S_w-YoCK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(Layer):\n",
        "\n",
        "  def __init__(self, embed_dim, num_heads, ff_dim): # ff_dim is Dimension of feed forward layer\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.att = MultiHeadAttention(embed_dim, num_heads)\n",
        "    self.ffn = tf.keras.Sequential([\n",
        "        Dense(ff_dim, activation=\"relu\"),\n",
        "        Dense(embed_dim),\n",
        "    ])\n",
        "    # y = (x - mean) / root(variance + epsilon).\n",
        "    # epsilon ensures we never divide by zero.\n",
        "    # it is small enough not to affect the result but large enough to prevent instability.\n",
        "    self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "    # Added two dropout layers - Drop neurons to avoid overfitting.\n",
        "    # 0.1 indicates the % of neurons we wish to drop.\n",
        "    self.dropout1 = Dropout(0.1)\n",
        "    self.dropout2 = Dropout(0.1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AEvYM6_a6kGh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}